{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading text document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Abstract\\nWith the advancement of Wi-Fi sensing technology, its significant benefits in\\nconvenient operation and privacy protection have become apparent, particularly\\nin fields like smart homes, medical monitoring, and security surveillance, where\\nthe application prospects of Human Activity Recognition (HAR) technology are\\nincreasingly broad. This study focuses on a novel approach to HAR using Wi-Fi\\nChannel State Information (CSI), especially under complex conditions such as\\nNon-Line of Sight (NLoS) paths and through-wall transmissions. Traditionally,\\nmost research has concentrated on Line of Sight (LoS) path HAR, sensitive to\\nenvironmental changes, while the NLoS path signals, especially through-wall signals, present unpredictability due to weak reflections caused by walls. Addressing\\nthis issue, we propose Wi-SensiNet, an innovative deep learning-based method\\nthat combines the spatial feature extraction capabilities of Convolutional Neural\\nNetworks (CNN) with the temporal sequence processing power of Bidirectional\\nLong Short-Term Memory networks (BiLSTM). This method also incorporates\\nan attention mechanism to enhance the accuracy of human activity recognition in complex environments. Wi-SensiNet is specially designed for through-wall\\nsettings, effectively handling the complexity of CSI data, and achieving accurate through-wall human activity detection. In our experiments, we collected\\na through-wall CSI dataset comprising seven common activities, including running, sitting, standing, squatting, falling, punching, and walking, and verified\\nWi-SensiNetâ€™s average accuracy exceeded 99% on the original test set. These\\nresults not only demonstrate the modelâ€™s robustness and high accuracy in handling HAR tasks in complex environments but also highlight the potential of\\nCNN and BiLSTM working in tandem to enhance performance.\\nKeywords: Human activity recognition, Deep learning, WiFi sensing, Through\\nwall,Channel state information.\\n\\nIntroduction\\nIn recent years, Human Activity Recognition (HAR) has increasingly found applications in fields such as smart homes, medical monitoring, and security surveillance.Particularly with the proliferation of smart devices and the advancement of\\nInternet of Things (IoT) technologies, Wi-Fi sensing technology has been increasingly\\napplied, harnessing a detectable feature within Wi-Fi signals known as Channel State\\nInformation (CSI).This type of data can intricately illustrate how wireless signals\\nare transmitted between sending and receiving devices, where movement of a person\\nwithin the Wi-Fi signal propagation path induces changes in the signalâ€™s phase and\\namplitude.These alterations are reflected in the CSI, a characteristic of Wi-Fi signals, offering detailed insights into the signal propagation path, transmission time,\\nand attenuation.Consequently, by analyzing the variations in CSI data, one can infer\\nchanges in human movement and positioning.Compared to traditional methods reliant\\non visual or wearable sensors, this approach offers a more covert and non-line-ofsight dependent solution.Wi-Fi-based Human Activity Recognition (HAR) is achieved\\nthrough the reception of both Signal Strength Information (RSSI) and Channel State\\nInformation (CSI).In comparison to RSS, CSI can provide detailed channel frequency\\nresponse information[1] on multiple channels at the physical layer, offering finer granularity than RSSI and the ability to distinguish multipath components, rendering it\\nmore effective in recognizing complex human movements.Owing to its high-resolution\\ncharacteristics, CSI has emerged as a pivotal technology in the HAR domain, particularly excelling in applications requiring precise capture and analysis of human\\ndynamics.\\nThe majority of solutions for Human Activity Recognition (HAR) based on\\nChannel State Information (CSI) in specific environments employ deep learning techniques, including gesture recognition[2][3], motion detection[4][5], and fall\\ndetection[6][7].However, researchers often conduct their studies in ideal environments\\nwith relatively simple Wi-Fi signal propagation. Due to the poor interference resistance of Wi-Fi signals, the accuracy of these solutions may significantly diminish\\nin more complex scenarios such as through-wall or Non-Line-of-Sight (NLOS) conditions.However, these complex scenarios often represent the primary application\\ncontexts for human activity recognition.The challenges encountered in human perception through walls using Wi-Fi include technical limitations such as signal attenuation,\\nmultipath interference, signal noise, environmental factors like wall composition,\\ndynamic environments, as well as privacy and ethical considerations.Consequently,\\nachieving high-accuracy human activity recognition in complex environments has\\nbecome a critical issue.Human Activity Recognition based on CSI fundamentally\\ninvolves analyzing the impact of human activity on wireless signals, suggesting that\\na potential solution to the aforementioned issues is to extract and utilize the most\\nrepresentative features within the wireless signals.Addressing these challenges is crucial for the advancement of this field.Robust algorithms, models, and frameworks\\nare required to mitigate the effects of signal issues and adapt to dynamic environments.Furthermore, the latest advancements in Multiple-Input Multiple-Output\\n(MIMO) communications and the utilization of Wi-Fi signals offer a promising avenue\\nto realize this goal.\\n\\nIn this study, we conducted an in-depth analysis of the original Channel State Information (CSI), with a particular focus on the impact of signal attenuation and minor\\nmovements on CSI in through-wall scenarios.To fully extract features, we treated the\\ndata from each antenna as an independent input channel, incorporating both timedomain and frequency-domain information.Additionally, we employed median filtering\\nto optimize data quality and reduce noise, coupled with the introduction of Gaussian\\nnoise enhancement to simulate uncertainties in real-world environments, effectively\\nmitigating noise interference caused by minor and inadvertent movements.These\\nstrategies collectively enhanced the reliability of the data, improving the modelâ€™s\\nperformance and adaptability in complex environments.\\nIn summary, this research makes the following key contributions:\\nâ€¢ In addressing the challenges of datasets in through-wall scenarios, we devised and\\napplied a data processing strategy combining median filtering with Gaussian noise,\\nwhich not only effectively reduced data noise but also significantly enhanced the\\nquality and generalizability of the data.Training with pre-processed data led to a\\nsubstantial increase in accuracy on the original dataset.\\nâ€¢ We introduced the Wi-SensiNet approach, which integrates Convolutional Neural\\nNetworks (CNN) with Bidirectional Long Short-Term Memory (BiLSTM) networks,\\nincorporating an attention mechanism to establish an advanced framework for\\ntime-series analysis. The specially tailored BiLSTM modules process deep features\\nextracted by the CNN layers, and the incorporated attention layer further optimizes\\nthe modelâ€™s recognition of critical temporal steps, enhancing the overall accuracy\\nand robustness of the model in understanding and predicting complex time-series\\ndata.\\nâ€¢ We collected and constructed a through-wall dataset encompassing nine common\\nhuman activities, including running, sitting, standing, squatting, falling, punching,\\nand standing.By applying the Wi-SensiNet method, we achieved an average accuracy\\nof 99% on the original dataset, validating the efficacy and precision of our approach\\nin practical applications. application.\\nThe remainder of this paper is structured as follows: Chapter 2 provides a comprehensive review of existing studies in the related field,Chapter 3 details the dataset\\nemployed in this research,Chapter 4 describes the experimental procedure and results,\\nand includes a comparative analysis with existing studies.Chapter 5 concludes the\\npaper with key findings and insights', metadata={'source': 'sample.txt'})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data_ingestion\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader=TextLoader(\"sample.txt\")\n",
    "text_documents=loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading API key from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading data from web using Beautiful soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n      LLM Powered Autonomous Agents\\n    Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\nFig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# load,chunk and index the contentof html page\n",
    "\n",
    "loader=WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                         class_=(\"post-title\",\"post-content\")\n",
    "                     )))\n",
    "\n",
    "web_document=loader.load()\n",
    "web_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Wi-SensiNet: Through-Wall Human Activity\\nRecognition Based on WiFi Sensing\\nFu Xiaoyi\\xa0\\nSouthwest University\\nWang Chenlu\\xa0\\nSouthwest University\\nLi Shenglin\\xa0 \\n \\nSouthwest University\\nResearch Article\\nKeywords:  Human activity recognition, Deep learning, WiFi se nsing, Through wall,Channel state\\ninformation\\nPosted Date:  March 19th, 2024\\nDOI:  https://doi.org/10.21203/rs.3.rs-4106293/v1\\nLicense:  \\uf25e \\uf4e7 This work is licensed under a Creative Commons Att ribution 4.0 International License. \\xa0\\nRead Full License\\nAdditional Declarations:  No competing interests reported.', metadata={'source': 'sample.pdf', 'page': 0}),\n",
       " Document(page_content='Wi-SensiNet: Through-Wall Human Activity\\nRecognition Based on WiFi Sensing\\nFu Xiaoyi1, Wang Chenlu1, Li Shenglin1*\\n1*College of Artiﬁcial Intelligence, Southwest University, Chongqing,\\n400715, China.\\n*Corresponding author(s). E-mail(s): lishenglin@swu.edu.cn ;\\nContributing authors: swufuxiaoyi@163.com ;\\nAbstract\\nWith the advancement of Wi-Fi sensing technology, its signi ﬁcant beneﬁts in\\nconvenient operation and privacy protection have become appare nt, particularly\\nin ﬁelds like smart homes, medical monitoring, and security surv eillance, where\\nthe application prospects of Human Activity Recognition (HA R) technology are\\nincreasingly broad. This study focuses on a novel approach to HAR using Wi-Fi\\nChannel State Information (CSI), especially under complex co nditions such as\\nNon-Line of Sight (NLoS) paths and through-wall transmission s. Traditionally,\\nmost research has concentrated on Line of Sight (LoS) path HAR, s ensitive to\\nenvironmental changes, while the NLoS path signals, especia lly through-wall sig-\\nnals, present unpredictability due to weak reﬂections caused b y walls. Addressing\\nthis issue, we propose Wi-SensiNet, an innovative deep learni ng-based method\\nthat combines the spatial feature extraction capabilities of C onvolutional Neural\\nNetworks (CNN) with the temporal sequence processing power of B idirectional\\nLong Short-Term Memory networks (BiLSTM). This method also inco rporates\\nan attention mechanism to enhance the accuracy of human activ ity recogni-\\ntion in complex environments. Wi-SensiNet is specially desi gned for through-wall\\nsettings, eﬀectively handling the complexity of CSI data, a nd achieving accu-\\nrate through-wall human activity detection. In our experiment s, we collected\\na through-wall CSI dataset comprising seven common activitie s, including run-\\nning, sitting, standing, squatting, falling, punching, and walking, and veriﬁed\\nWi-SensiNet’s average accuracy exceeded 99% on the original te st set. These\\nresults not only demonstrate the model’s robustness and high acc uracy in han-\\ndling HAR tasks in complex environments but also highlight th e potential of\\nCNN and BiLSTM working in tandem to enhance performance.\\nKeywords: Human activity recognition, Deep learning, WiFi sensing, Th rough\\nwall,Channel state information\\n1', metadata={'source': 'sample.pdf', 'page': 1}),\n",
       " Document(page_content='1 Introduction\\nIn recent years, Human Activity Recognition (HAR) has increasingly found ap pli-\\ncations in ﬁelds such as smart homes, medical monitoring, and security s urveil-\\nlance.Particularly with the proliferation of smart devices and the adv ancement of\\nInternet of Things (IoT) technologies, Wi-Fi sensing technology has be en increasingly\\napplied, harnessing a detectable feature within Wi-Fi signals kno wn as Channel State\\nInformation (CSI).This type of data can intricately illustrate how w ireless signals\\nare transmitted between sending and receiving devices, where m ovement of a person\\nwithin the Wi-Fi signal propagation path induces changes in the signal’s p hase and\\namplitude.These alterations are reﬂected in the CSI, a characteris tic of Wi-Fi sig-\\nnals, oﬀering detailed insights into the signal propagation path, transmi ssion time,\\nand attenuation.Consequently, by analyzing the variations in CSI data, on e can infer\\nchanges in human movement and positioning.Compared to traditional metho ds reliant\\non visual or wearable sensors, this approach oﬀers a more covert and non-lin e-of-\\nsight dependent solution.Wi-Fi-based Human Activity Recognition (HAR) is achieved\\nthrough the reception of both Signal Strength Information (RSSI) and Chann el State\\nInformation (CSI).In comparison to RSS, CSI can provide detailed chan nel frequency\\nresponse information[ 1] on multiple channels at the physical layer, oﬀering ﬁner gran-\\nularity than RSSI and the ability to distinguish multipath component s, rendering it\\nmore eﬀective in recognizing complex human movements.Owing to it s high-resolution\\ncharacteristics, CSI has emerged as a pivotal technology in the HAR domain, p ar-\\nticularly excelling in applications requiring precise capture an d analysis of human\\ndynamics.\\nThe majority of solutions for Human Activity Recognition (HAR) based on\\nChannel State Information (CSI) in speciﬁc environments employ de ep learn-\\ning techniques, including gesture recognition[ 2][3], motion detection[ 4][5], and fall\\ndetection[ 6][7].However, researchers often conduct their studies in ideal envi ronments\\nwith relatively simple Wi-Fi signal propagation. Due to the poor inte rference resis-\\ntance of Wi-Fi signals, the accuracy of these solutions may signiﬁcantly diminish\\nin more complex scenarios such as through-wall or Non-Line-of-Sight (NLOS) con -\\nditions.However, these complex scenarios often represent the pri mary application\\ncontexts for human activity recognition.The challenges encountered i n human percep-\\ntion through walls using Wi-Fi include technical limitations such as signal attenuation,\\nmultipath interference, signal noise, environmental factors like w all composition,\\ndynamic environments, as well as privacy and ethical considerations.C onsequently,\\nachieving high-accuracy human activity recognition in complex environ ments has\\nbecome a critical issue.Human Activity Recognition based on CSI fundame ntally\\ninvolves analyzing the impact of human activity on wireless signals, su ggesting that\\na potential solution to the aforementioned issues is to extract and uti lize the most\\nrepresentative features within the wireless signals.Addressin g these challenges is cru-\\ncial for the advancement of this ﬁeld.Robust algorithms, models, and fr ameworks\\nare required to mitigate the eﬀects of signal issues and adapt to dynamic envi-\\nronments.Furthermore, the latest advancements in Multiple-Inp ut Multiple-Output\\n(MIMO) communications and the utilization of Wi-Fi signals oﬀer a promis ing avenue\\nto realize this goal.\\n2', metadata={'source': 'sample.pdf', 'page': 2}),\n",
       " Document(page_content='Inthisstudy,weconductedanin-depthanalysisoftheoriginalChanne lStateInfor-\\nmation (CSI), with a particular focus on the impact of signal attenuation an d minor\\nmovements on CSI in through-wall scenarios.To fully extract feature s, we treated the\\ndata from each antenna as an independent input channel, incorporating b oth time-\\ndomain and frequency-domain information.Additionally, we employed me dian ﬁltering\\nto optimize data quality and reduce noise, coupled with the introdu ction of Gaussian\\nnoise enhancement to simulate uncertainties in real-world environ ments, eﬀectively\\nmitigating noise interference caused by minor and inadvertent move ments.These\\nstrategies collectively enhanced the reliability of the data, impro ving the model’s\\nperformance and adaptability in complex environments.\\nIn summary, this research makes the following key contributions:\\n•In addressing the challenges of datasets in through-wall scenarios, we d evised and\\napplied a data processing strategy combining median ﬁltering with Gaussian noise,\\nwhich not only eﬀectively reduced data noise but also signiﬁcantly e nhanced the\\nquality and generalizability of the data.Training with pre-processe d data led to a\\nsubstantial increase in accuracy on the original dataset.\\n•We introduced the Wi-SensiNet approach, which integrates Convoluti onal Neural\\nNetworks (CNN) with Bidirectional Long Short-Term Memory (BiLSTM) netw orks,\\nincorporating an attention mechanism to establish an advanced framework f or\\ntime-series analysis. The specially tailored BiLSTM modules proc ess deep features\\nextracted by the CNN layers, and the incorporated attention layer fur ther optimizes\\nthe model’s recognition of critical temporal steps, enhancing the ov erall accuracy\\nand robustness of the model in understanding and predicting comple x time-series\\ndata.\\n•We collected and constructed a through-wall dataset encompassing nine common\\nhuman activities, including running, sitting, standing, squattin g, falling, punching,\\nandstanding.ByapplyingtheWi-SensiNetmethod,weachievedanave rageaccuracy\\nof 99% on the original dataset, validating the eﬃcacy and precision of our approach\\nin practical applications. application.\\nThe remainder of this paper is structured as follows: Chapter 2 prov ides a compre-\\nhensive review of existing studies in the related ﬁeld,Chapter 3 details the dataset\\nemployed in this research,Chapter 4 describes the experiment al procedure and results,\\nand includes a comparative analysis with existing studies.Chapter 5 concludes the\\npaper with key ﬁndings and insights.\\n2 Related works\\nIn this chapter, we conduct a thorough review of the major works relate d to our study,\\nincluding the application of Channel State Information (CSI) data, Human Ac tivity\\nRecognition (HAR) technologies in Non-Line-of-Sight (NLOS) environments, an d the\\napplication of deep learning in this ﬁeld.In 2000, Bahl et al. [ 8] introduced Radar,\\nan innovative system for indoor positioning based on Wi-Fi Received S ignal Strength\\n(RSS), marking a pioneering use of Wi-Fi signals for sensing purpose s.Subsequently,\\nCSIdatabecameaccessibleinamultitudeofcommercialdevices,wi thprominenttools\\n3', metadata={'source': 'sample.pdf', 'page': 3}),\n",
       " Document(page_content='such as the Intel 5300 NIC [ 9], Atheros CSI Tool[ 10], and Nexmon CSI Tool[ 11] emerg-\\ning.Thesetoolsfacilitatedthecreationofnumerousplatforms,signiﬁ cantly simplifying\\nthe process of CSI data collection.CSI data, being a signal rich in env ironmental\\ninformation, has been extensively utilized for indoor activity moni toring.\\nWang et al. developed the CARM system[ 12], a human activity recognition and\\nmonitoring system based on Channel State Information (CSI), grounded in two the-\\noretical models: the CSI-Speed Model and the CSI-Activity Model . This system\\ncorrelates CSI dynamics with human movement speeds and associates t hese speeds\\nwith speciﬁc activities.This model-based approach, however, ex hibits limitations in\\nenvironmental adaptability, diversity in activity recognition, real -time processing\\neﬃciency, generalization capabilities, and in handling complex data.W ith the advance-\\nment of deep learning, researchers are increasingly leveraging this t echnology for\\nmotion recognition.Ma et al. introduced the SignFi method[ 13], a system architecture\\nprimarily based on the Channel State Information (CSI) of Wi-Fi signals, employ-\\ning Convolutional Neural Networks (CNN) for the classiﬁcation and recognition of\\nsign language gestures.The system analyzes Wi-Fi signal variations caused by ges-\\nture movements to extract features of sign language.Shi et al. describ ed a method\\nfor human activity recognition in their study[ 14].This method initially employs an\\nalgorithm to preprocess CSI signals, enhancing activity-related si gnals and reducing\\nnoise.Subsequently,itextractsfeaturesbycomputingcorrelati onsinbothtimeandfre-\\nquency domains.Utilizing these processed signals, the method aut omatically extracts\\ndeeper features through a deep learning model.\\nWhileexploringWi-Fi-basedmotionrecognitiontechnologies,researc hershavealso\\nexpanded their focus to more challenging environments, leveraging the through-wall\\ncapabilities of Wi-Fi signals for human activity recognition in such sce narios.Wang\\net al.[15] introduced a method utilizing Device-Free Sensing (DFS) t echnology in\\ncomplex scenarios, with a particular emphasis on its application in thr ough-wall\\nand Non-Line-of-Sight (NLOS) environments.The paper proposed a novel st rategy to\\nenhance DFS system performance by utilizing spatial structural in formation, inte-\\ngrating multi-dimensional features across time, frequency, and spat ial domains for\\nmore accurate identiﬁcation of target positions and activities.In conduc ting research\\non through-wall activity recognition using commercial Wi-Fi devices, we encounter\\nseveral challenges.In this paper, we propose a deep learning-based app roach that\\neﬀectively mitigates the impact of noise and environmental factors.Th is approach\\ndemonstrates strong generalization capabilities, introducing a nov el method to the\\nﬁeld of through-wall activity recognition.\\n3 Dataset\\nTo ensure the quality of the collected data and the generalization abili ty of the model,\\nwe initially conducted a series of preprocessing steps on the origi nal dataset.This\\nincluded the application of various noise reduction techniques to mi nimize noise inter-\\nference in the data, as well as the implementation of data augmentation str ategies to\\noptimize the dataset and increase sample diversity.Noise reduction p rocessing aims to\\nenhance the clarity of data signals, thereby enabling the model to mor e accurately\\n4', metadata={'source': 'sample.pdf', 'page': 4}),\n",
       " Document(page_content='capture subtle variations in human movements.Data augmentation, by si mulating\\npotential variations and disturbances, enhances the model’s adaptabil ity to new sce-\\nnarios and conditions.In the following sections, we will provide a d etailed description\\nof the noise reduction and data augmentation methods employed.\\n(a) Non-through-wall action\\n (b) Through-wall actionn\\nFig. 1: Comparison of amplitude values after normalization and variance processi ng.\\nThe horizontal axis represents the variation in time, while the vert ical axis indicates\\nthe changes in amplitude data post-normalization and variance processi ng.\\n3.1 Original Dataset\\nThe original dataset was collected using the Intel 5300 NIC, which deﬁnes the Channel\\nState Information (CSI)[ 15] as a function, given by the formula\\nHc,m\\nt=Ac,m\\ntejφc,m\\nt (1)\\nHere,Hc,m\\nt,Ac,m\\ntandφc,m\\ntrepresentthecomplexchannelresponse,amplituderesponse,\\nand phase response, respectively, for channel con antenna mat timet.In the original\\ndataset collected for this study, the dimensions of each data sample ar e deﬁned by the\\nequation Xt=NT×NR×Nsub, whereNT,NR,Nsubdenote the number of transmit-\\nting antennas, receiving antennas, and subcarriers on each antenna, re spectively.For\\nthe through-wall Channel State Information (CSI) dataset, due to the com plexity\\nof through-wall environments, the original CSI data is frequently sub ject to vari-\\nous interferences, such as signal attenuation, noise disturbances, an d environmental\\nchanges.\\nAs illustrated in Fig. 1, we utilize Normalized Variance (NV) to evaluate both non-\\nthrough-wall and through-wall data.To accentuate the diﬀerences betwe en the two\\nscenarios, we apply normalized variance analysis to through-wall and non-t hrough-\\nwall data within a speciﬁc action time window.Fig. 1ademonstrates the amplitude\\nvariations caused by a regular standing action, whereas Fig. 1bexhibits the amplitude\\n5', metadata={'source': 'sample.pdf', 'page': 5}),\n",
       " Document(page_content='(a)\\n (b)\\nFig. 2: Comparison of data before and after preprocessing. In the visual repre senta-\\ntion, (a) denotes the original data, while ( b) represents the data post-preprocessing.\\nThe x-axis signiﬁes the carrier index, the y-axis denotes the packe t index, and the z-\\naxis corresponds to the amplitude.\\nvariations of the same action under through-wall conditions.The distinc tion between\\nthe two is evident; the through-wall data exhibits a greater normaliz ed variance, indi-\\ncating an increase in the volatility of the channel state. The calculati on formula for\\nNormalized Variance (NV) is as follows:\\nNV=σ2\\nµ2(2)\\nhere,σ2represents the variance, calculated as:\\nσ2=1\\nN−1N∑\\ni=1(xi−µ)2(3)\\nµdenotes the sample mean, computed as:\\nµ=1\\nNN∑\\ni=1xi (4)\\nHere,xirepresents an individual sample value, and Ndenotes the number of sam-\\nples.Utilizing this formula allows for the quantiﬁcation of channel q uality variations\\nunder through-wall and non-through-wall conditions.The observation of a h igher\\nNormalized Variance in through-wall scenarios indicates increased signal volatility,\\npotentially caused by multipath eﬀects from walls or other environmen tal factors.This\\nincrease in volatility suggests the need for appropriate data preproce ssing strategies\\n6', metadata={'source': 'sample.pdf', 'page': 6}),\n",
       " Document(page_content='to adapt to such changes. Preprocessing may include ﬁltering, den oising, or employ-\\ning more sophisticated signal processing techniques to stabilize the signal, thereby\\nenhancing the accuracy and reliability of subsequent analyses.\\n3.2 Data Preprocessing\\nInthisstudy,weemployedamedianﬁlterfornoisereductioninCh annelStateInforma-\\ntion (CSI) data.A non-linear digital ﬁltering technique, often used in signal processing\\nto reduce noise. This method preserves edges while removing noi se, by replacing each\\nentry with the median of neighboring entries.Median ﬁlters are high ly advantageous in\\nimageandsignalprocessing,particularlyeﬀectiveineliminatingsal t-and-peppernoise,\\nwhile eﬃciently preserving the edge characteristics of images.As a nonlinear ﬁltering\\ntechnique, it is less sensitive to outliers, making it particul arly eﬀective in processing\\nsignals with anomalies or noise.The fundamental equation for median ﬁlter ing is\\ny(i) = Med{x(i−k),...,x(i+k)} (5)\\nwherex(i) represents the original signal, y(i) is the signal post-ﬁltering, and kis\\nhalf the window size. In this context, we have set the window siz e of the median ﬁlter\\nto 3x3, to eﬀectively eliminate noise while preserving the edge fe atures of the signal.\\nAdditionally, considering the potential attenuation of features due to through-wall\\nsignals, we incorporated Gaussian noise augmentation to enhance the featur e repre-\\nsentation of the dataset.Adding Gaussian Noise is a process in signal proc essing where\\nGaussian noise (a statistical noise having a probability density fun ction equal to that\\nof the normal distribution) is added to a signal. This technique is com monly used in\\nalgorithmstotestrobustnessagainstnoiseortoimprovegeneralization. Theadditionof\\nGaussian noise follows the principle of z(i) =x(i)+N(µ,σ2) In this context,, N(µ,σ2)\\nrepresents a Gaussian distribution with a mean of µand a variance denoted by ε=\\nσ2.In our experiments, we set the mean of the Gaussian noise to 0 and the s tandard\\ndeviation to 0.1, ensuring that the noise level was moderate and did not ob scure the\\ncharacteristics of the original signal.This approach not only simulates th e uncertain-\\nties of real-world environments but also enhances the data’s represe ntational capacity\\nwhile preserving the characteristics of the original signal, making i t particularly suit-\\nable for through-wall signal processing scenarios.As demonstrated in Fi g.2, we present\\na comparison of the data before and after preprocessing.\\n4 System Architecture\\nIn this study, a deep learning-based algorithmic framework is devel oped, aimed at\\nprocessing and classifying Channel State Information (CSI) data coll ected through\\nwalls. As depicted in Fig. 4, the entire processing pipeline is segmented into sev-\\neral pivotal stages: Initially, the preprocessed through-wall CSI dat a is fed into a\\nConvolutional Neural Network (CNN) module. This module leverages its mul tiple con-\\nvolutional layers to autonomously extract spatial features from the data. T he features\\noutputted by the CNN module are then reshaped to conform to the input speciﬁca-\\ntions of subsequent modules. Subsequently, the reshaped time- series data is input into\\n7', metadata={'source': 'sample.pdf', 'page': 7}),\n",
       " Document(page_content='a Bidirectional Long Short-Term Memory (BiLSTM) module. The archite cture of the\\nBiLSTM module is meticulously designed to process time-series data, with a speciﬁc\\nfocus on capturing extended dependencies in the temporal dimens ion. Following this,\\nthe output from the BiLSTM module is conveyed to a fully connected l ayer, which is\\ntasked with mapping the temporal features onto speciﬁc categories of act ions.\\nTo augment the model’s proﬁciency in interpreting time-series data, an attention\\nmechanism has been integrated. This mechanism dynamically modulate s the model’s\\nfocus across various time steps in the sequence. By assigning atten tion weights to the\\nhidden states of the Bidirectional Long Short-Term Memory (BiLSTM) mo dule, the\\nmodel intensively processes dynamic features that are essential for the classiﬁcation\\ntask. This enhancement notably escalates the model’s accuracy in rec ognizing human\\nactivities through walls, especially in environments with complex signal patterns.\\nConsequently, the proposed CNN-BiLSTM framework, enriched with the attention\\nmechanism, amalgamates the strengths of spatial feature extraction and tem poral\\ndependency capture. Furthermore, it attains an in-depth comprehe nsion of time-series\\ndata via the incorporation of the attention model. This culminates in a novel and\\neﬃcacious methodology for human activity recognition in through-wall scen arios.\\n4.1 Convolutional Neural Network\\nIn our research, for the three-dimensional through-wall Channel Stat e Informa-\\ntion (CSI) dataset, we employed a two-dimensional Convolutional Neural Ne twork\\n(CNN2D) module to perform feature extraction tasks.This dataset posses ses dimen-\\nsions of 3x500x30, corresponding respectively to the number of antennas, t ime steps,\\nand amplitude information for each antenna.The design of the CNN2D module sp eciﬁ-\\ncally accounts for the unique structure of CSI data, aiming to eﬃcient ly extract spatial\\nfeatures embedded within the time-series data.\\nAs depicted in Fig. 3, the CNN2D module comprises two convolutional layers, each\\nfollowed by a Rectiﬁed Linear Unit (ReLU) and a max pooling layer.The ﬁr st con-\\nvolutional layer utilizes 3x3 kernels (with a stride of 1 and padding of 1) to process\\nsignals from diﬀerent antennas, generating feature maps containing pri mary spatial\\nfeatures.Subsequently, the ReLU activation function is applied to introduce nonlin-\\nearity, aiding in capturing more complex data patterns. Subsequent ly, a max pooling\\nlayer with 2x2 kernels and a stride of 2 is utilized to reduce the s patial dimensions of\\nConv2D Conv2D MAX-POOL MAX-POOL\\nFig. 3: Schematic Diagram of the CNN Module.\\n8', metadata={'source': 'sample.pdf', 'page': 8}),\n",
       " Document(page_content='Fig. 4: Algorithm process framework.\\nthe feature maps while retaining essential features.This downsam pling step is aimed\\nat reducing computational load and preventing overﬁtting.The second convolutional\\nlayer further processes the features, and another ReLU activation fu nction is employed\\nto maintain nonlinearity.Then, max pooling is applied again to further reduce the\\nsize of the feature maps, facilitating higher-level feature abstract ion.The design of\\nthis CNN module takes into account the uniqueness of through-wall CSI data. With\\ncarefully selected parameters and layer structure, it ensures t hat the network eﬀec-\\ntively extracts crucial spatiotemporal features from the data, signiﬁc antly enhancing\\nthe accuracy and eﬃciency of subsequent classiﬁcation tasks.\\n4.2 BiLSTM\\nIn the architecture we propose, a Bidirectional Long Short-Term Memor y network\\n(BiLSTM) is employed following the Convolutional Neural Network (CNN) mo dule to\\neﬀectively process sequential data. As illustrated in Fig. 5,the BiLSTM enhances the\\nlearningofsequencefeaturesbycapturingthecontextualinformati onofthetimeseries\\nfrom both forward and backward directions simultaneously. In its speci ﬁc implemen-\\ntation, the BiLSTM consists of a single LSTM layer, with the input fe ature dimension\\nspeciﬁed as the reshaped output of the preceding layer, while the dimension of the\\nhidden layer is set to 64. Furthermore, we ensure that the batch siz e of the input\\nand output tensors is positioned in the ﬁrst dimension. The bidirec tional processing\\ncapability of the LSTM is also activated. This forms a comprehensive fr amework for\\n9', metadata={'source': 'sample.pdf', 'page': 9}),\n",
       " Document(page_content='analyzing time-series data. The LSTM module is renowned for its eﬃc iency in cap-\\nturing long-term dependencies in sequential data and is speciﬁcal ly tailored to handle\\nthe feature-rich outputs extracted by the preceding CNN layer. A t each time step t,\\nthe forward LSTM segment computes the hidden state− →htusing the current input xt\\nand the hidden state−−→ht−1from the previous time step, while the backward LSTM\\ncalculates← −htbased on the same input and the hidden state←−−ht+1from the subsequent\\ntime step. The output htof the bidirectional LSTM at each time step is a concate-\\nnation of the forward and backward hidden states, formulated as ht= [− →ht;← −ht]. This\\nstructure allows the model to integrate the information of the entir e input sequence\\nat each time step, thus more comprehensively capturing the long-ter m dependencies\\nin the sequence.\\n4.3 Attention mechanism\\nAdditionally, our model incorporates an attention mechanism, aimed at fur ther\\nenhancing the identiﬁcation of key sequential information. This att ention mechanism\\nassignsaweighttoeachtimestepoutputofthebidirectionalLSTM,the rebyemphasiz-\\ning more signiﬁcant features while suppressing less relevant inf ormation. Speciﬁcally,\\nwe initially transform the BiLSTM output at each time step into a scalar using a linear\\nlayer, a step that can be viewed as scoring the importance of each time s tep. Subse-\\nquently, we apply a softmax function to normalize these scores, ensu ring that the sum\\nFig. 5: BILSTM with Integrated Attention Mechanism.\\n10', metadata={'source': 'sample.pdf', 'page': 10}),\n",
       " Document(page_content='Fig. 6: System Workﬂow.\\nof attention weights across all time steps equals 1. This process can be r epresented as:\\nat= softmax( Wattn·ht+battn) (6)\\nHere,atrepresents the attention weight for time step t, whileWattnandbattnare the\\nweight and bias of the linear layer, respectively, and htis the output of the BiLSTM\\nat time step t. Subsequently, we compute the context vector ctas the sum of the\\nweighted BiLSTM outputs:\\nc=T∑\\nt=1at·ht (7)\\nThe context vector cprovides a weighted representation of the sequence, where the\\ncontribution of each time step is determined based on its relative im portance. This\\nenables the model to utilize more reﬁned and targeted sequence feat ures in subsequent\\nfully connected layers.\\n5 Experiments And Results\\nIn this section, we detail our experimental procedures and the corr esponding results.\\n5.1 Dataset Collection\\nWe meticulously designed and constructed a data collection experim ent speciﬁcally\\ntargeting through-wall propagation scenarios, aimed at augmenting our unders tanding\\nand simulation of wireless signal behavior in real-world environments .As depicted in\\nFig.7, the transmitter (TX) is strategically placed in an outdoor setting to emulate\\nthe reception of wireless signals within a home or oﬃce environment.C onversely, the\\nreceiver(RX)ispositionedindoors,ensuringthatthesignalsitre ceivesmustpenetrate\\nthe walls of the building.\\n11', metadata={'source': 'sample.pdf', 'page': 11}),\n",
       " Document(page_content='Fig. 7: Dataset collection scenario.\\nDuring the experimental phase, the participants engaged in a variety of pre-deﬁned\\nmotions within an indoor setting, encompassing fundamental activitie s like walking,\\nrunning, and boxing, along with a range of everyday actions. This approach w as\\ndesigned to maximize the coverage of typical human activities within the collected\\ndata. Each activity was meticulously labeled and linked with concurre nt wireless sig-\\nnal data collection. The primary objective was to furnish a comprehen sive training\\nand testing dataset for the development of advanced motion classiﬁcation al gorithms.\\nWithin the experimental setup, particular emphasis was placed on th e pathways\\nof wireless signal propagation and potential environmental obstacles. Key p arameters,\\nincluding the direct line distance between the transmitter and receiver, the material\\nproperties and thickness of walls, as well as the relative indoor and out door position-\\ning, were meticulously documented and analyzed. Furthermore, in or der to simulate\\nthe propagation of wireless signals in varied residential and occupational settings more\\naccurately, the positions of the subjects were varied in accordance wi th their move-\\nments to assess the impact of diverse factors on the behavior of wirele ss signals.\\nEach activity was accurately tagged and associated with the simultaneous ly collected\\nwireless signal data, with the aim of enriching the training and testi ng datasets for\\nforthcoming motion classiﬁcation algorithms.\\n12', metadata={'source': 'sample.pdf', 'page': 12}),\n",
       " Document(page_content='(a) Performance on the test set post-\\npreprocessing\\n(b) Performance on the test set with original\\ndata\\nFig. 8: Confusion Matrix for Through-Wall Activity Recognition.The horizontal ax is\\nrepresents predicted labels, while the vertical axis denotes th e true labels.\\n5.2 System Workﬂow\\nIn our study, as illustrated in Fig. 6, we employed a human activity recognition system\\nbased on Channel State Information (CSI) data.The system’s workﬂow enc ompasses\\nseveral stages: initially acquiring raw CSI data through collection de vices, followed\\nby data preprocessing involving denoising and data augmentation to en hance data\\nquality.Subsequently, the preprocessed data is fed into a dee p learning model that\\nintegrates a Convolutional Neural Network (CNN) with a Bidirectional Long Shor t-\\nTerm Memory (BiLSTM) network equipped with an attention mechanism, responsible\\nfor feature extraction and time series analysis.Ultimately, the mode l outputs a prob-\\nability distribution for action classiﬁcation, achieving precise id entiﬁcation of human\\nactivities.\\n00.20.40.60.811.2\\nbox fall run sit squat stand walk\\nWi-SensiNet\\n CNN\\nLSTM\\nFig. 9: Accuracy of Three Models in Recognizing Various Actions.\\n13', metadata={'source': 'sample.pdf', 'page': 13}),\n",
       " Document(page_content='5.3 Experimental Results\\nIn this section, we will present a detailed exposition of the resul ts obtained from our\\nexperimental study.The experiment is designed to validate the e ﬃcacy of our pro-\\nposed model in classifying human activities.By comparing the perf ormance of the\\nmodel under diﬀerent conﬁgurations, we are able to accurately assess t he impact of\\nvarious factors on the classiﬁcation accuracy. As illustrated in Fig. 8, we conducted\\na comparative analysis of the model’s performance on both preprocessed and origi-\\nnal datasets.The results of the confusion matrix clearly demonstrate t hat the model\\ntrained on datasets subjected to noise reduction and data augmentation ex hibits supe-\\nrior classiﬁcation accuracy on the original dataset.Notably, the diagonal eleme nts,\\nrepresenting the percentage of correct classiﬁcations, are generally higher in the confu-\\nsion matrix of the preprocessed dataset compared to the original dataset. This ﬁnding\\nvalidates the eﬀectiveness of our data preprocessing strategy, aﬃr ming its signiﬁcant\\nrole in enhancing the model’s generalization capability on unseen data. Fig.9presents\\nTable 1:ComparisonofwallpenetratingHARusingmachinelearningandrecogniti on\\ntechniques.\\nName Method Activity Hardware Accuracy\\nWi-SensiNet CNN-\\nBiLSTM(attention)Running, Sitting,\\nStanding, Squatting,\\nFalling, Punching,\\nWalkingWiFi devices 99%\\nTW-See[ 16] BP-network Walking, falling,\\nwaving, boxing,\\nstanding up, sitting\\ndown, emptyWiFi devices 94.46%\\nWiHACS[ 17] SVM(one wall) Sitting, standing,\\nwalking, squatting,\\nfalling, lying down,\\nstanding up after lyingWiFi devices 92%\\nRbHAR[ 18] Adaptive\\nthresholdingwalking, sitting,\\nstanding, picking up an\\nobject, drinking,fallingRadar sensors 93%\\na comparative analysis of the accuracy of three distinct models in recogn izing seven\\ndiﬀerent activities.These activities include boxing, falling, running, sitting, squatting,\\nstanding, and walking.The three models are Wi-SensiNet, CNN, and LSTM re spec-\\ntively. It is observable that, in most activity recognition tasks, the accuracy of the\\nWi-SensiNet model slightly surpasses the other two models.Parti cularly in the recogni-\\ntion of falling activity, the Wi-SensiNet model demonstrates a signi ﬁcant advantage.In\\ncontrast, the performance of CNN and LSTM is relatively similar across all ac tivities,\\nthough CNN exhibits a slight edge in recognizing squatting and standing actions.This\\nchart signiﬁcantly highlights the performance disparities among diﬀe rent models in\\nactivity recognition tasks and provides a quantitative basis for furth er discussion and\\nanalytical research.\\n14', metadata={'source': 'sample.pdf', 'page': 14}),\n",
       " Document(page_content='Fig. 10: Accuracy performance of diﬀerent datasets.\\nAs depicted in Table 1, we conducted a comparative analysis with other literature\\nthat utilizes machine learning and recognition techniques for through -wall Human\\nActivity Recognition (HAR).These approaches include systems based on W iFi devices\\nand radar sensors, each targeting the recognition of various daily activiti es.By con-\\ntrasting these various methods, we assessed their respective pe rformances, and the\\ncomparison has illuminated the advancements made in the ﬁeld of human act ivity\\nrecognition through diﬀerent technological approaches, also oﬀering valuab le insights\\nfor our research.\\nAsdepictedinFig. 10,wepresentacomparisonoftheperformanceofthreemodels:\\nWi-SensiNet, CNN, and LSTM across diﬀerent datasets and preprocessing methods.\\nNotably, the Wi-SensiNet model consistently outperforms the other m odels across all\\ndatasets, as indicated by the higher accuracy metric. Additionally, th e preprocessing\\nmethod combining median ﬁltering with Gaussian noise enhancement appears to sig-\\nniﬁcantly improve model generalization, as reﬂected by the increas ed accuracy rates\\nacross datasets when this method is employed. Furthermore, the rob ustness of the Wi-\\nSensiNet model is evident from its sustained high accuracy irrespe ctive of the dataset\\npartitioning rules applied. This graph eloquently demonstrates th e superiority of the\\nWi-SensiNet model in terms of accuracy and generalization capabilities , as well as its\\nrobustness to diﬀerent data division strategies.\\n6 Conclusion\\nIn summary, this study developed an innovative human activity recogn ition system\\nbased on Channel State Information (CSI), speciﬁcally targeting activi ty detection\\nunder Non-Line-of-Sight (NLOS) conditions. The system employs state- of-the-art\\n15', metadata={'source': 'sample.pdf', 'page': 15}),\n",
       " Document(page_content='signal processing techniques, signiﬁcantly enhancing the predi ctive capability for\\nthrough-wall human activity recognition using WiFi sensing technology . The primary\\nexperimental focus was on minimizing the impact of prediction error s through walls\\non accuracy in indoor environments. Data denoising and enhancement st rategies were\\nemployed, optimizing the quality of the dataset and boosting the mod el’s adaptability\\nto new environments. Technically, the model amalgamates the spatial f eature extrac-\\ntion prowess of Convolutional Neural Networks (CNN) with the temporal analysi s\\nproﬁciency of Bidirectional Long Short-Term Memory (BiLSTM) networks , further\\naugmented by an attention mechanism. The experimental results unde rscore the high\\naccuracy of this approach in human activity recognition, demonstrating it s viability\\nin practical applications.\\nReferences\\n[1] Zhou, Z., Wu, C., Yang, Z., Liu, Y.: Sensorless sensing with wiﬁ. T singhua Science\\nand Technology 20(1), 1–6 (2015)\\n[2] Yang, J., Zou, H., Zhou, Y., Xie, L.: Learning gestures from wiﬁ: A siamese\\nrecurrent convolutional architecture. IEEE Internet of Things Jour nal6(6),\\n10763–10772 (2019)\\n[3] Li, C., Liu, M., Cao, Z.: Wihf: Enable user identiﬁed gesture recogn ition with\\nwiﬁ. In: IEEE INFOCOM 2020-IEEE Conference on Computer Communications,\\npp. 586–595 (2020). IEEE\\n[4] Ding, X., Jiang, T., Zhong, Y., Wu, S., Yang, J., Xue, W.: Improving wiﬁ -based\\nhuman activity recognition with adaptive initial state via one-shot lear ning. In:\\n2021 IEEE Wireless Communications and Networking Conference (WCNC), pp .\\n1–6 (2021). IEEE\\n[5] Zou, H., Zhou, Y., Yang, J., Jiang, H., Xie, L., Spanos, C.J.: Deepsense: D evice-\\nfreehumanactivityrecognitionviaautoencoderlong-termrecurren tconvolutional\\nnetwork. In: 2018 IEEE International Conference on Communications (ICC ), pp.\\n1–6 (2018). IEEE\\n[6] Palipana, S., Rojas, D., Agrawal, P., Pesch, D.: Falldeﬁ: Ubiquitous fall detection\\nusing commodity wi-ﬁ devices. Proceedings of the ACM on Interacti ve, Mobile,\\nWearable and Ubiquitous Technologies 1(4), 1–25 (2018)\\n[7] Ding, J., Wang, Y.: A wiﬁ-based smart home fall detection system usin g recur-\\nrent neural network. IEEE Transactions on Consumer Electronics 66(4), 308–317\\n(2020)\\n[8] Bahl, P., Padmanabhan, V.N.: Radar: An in-building rf-based user locat ion and\\ntrackingsystem.In:ProceedingsIEEEINFOCOM2000.ConferenceonComp uter\\nCommunications. Nineteenth Annual Joint Conference of the IEEE Comput er\\n16', metadata={'source': 'sample.pdf', 'page': 16}),\n",
       " Document(page_content='and Communications Societies (Cat. No. 00CH37064), vol. 2, pp. 775–784 (2000).\\nIeee\\n[9] Halperin, D., Hu, W., Sheth, A., Wetherall, D.: Tool release: Gathe ring 802.11 n\\ntraces with channel state information. ACM SIGCOMM computer communi ca-\\ntion review 41(1), 53–53 (2011)\\n[10] Xie, Y., Li, Z., Li, M.: Precise power delay proﬁling with commodi ty wiﬁ. In:\\nProceedings of the 21st Annual International Conference on Mobile Computi ng\\nand Networking, pp. 53–64 (2015)\\n[11] Gringoli, F., Schulz, M., Link, J., Hollick, M.: Free your csi: A c hannel state infor-\\nmation extraction platform for modern wi-ﬁ chipsets. In: Proceedin gs of the 13th\\nInternational Workshop on Wireless Network Testbeds, Experimental E valuation\\n& Characterization, pp. 21–28 (2019)\\n[12] Wang, W., Liu, A.X., Shahzad, M., Ling, K., Lu, S.: Device-free human act ivity\\nrecognition using commercial wiﬁ devices. IEEE Journal on Selected Ar eas in\\nCommunications 35(5), 1118–1131 (2017)\\n[13] Ma, Y., Zhou, G., Wang, S., Zhao, H., Jung, W.: Signﬁ: Sign language recogni-\\ntion using wiﬁ. Proceedings of the ACM on Interactive, Mobile, Wearab le and\\nUbiquitous Technologies 2(1), 1–21 (2018)\\n[14] Shi, Z., Zhang, J.A., Xu, R., Cheng, Q.: Deep learning networks for h uman activ-\\nity recognition with csi correlation feature extraction. In: ICC 2019-2019 IE EE\\nInternational Conference on Communications (ICC), pp. 1–6 (2019). IEEE\\n[15] Wang, J., Zhang, L., Gao, Q., Pan, M., Wang, H.: Device-free wireless se nsing\\nin complex scenarios using spatial structural information. IEEE Trans actions on\\nWireless Communications 17(4), 2432–2442 (2018)\\n[16] Wu, X., Chu, Z., Yang, P., Xiang, C., Zheng, X., Huang, W.: Tw-see: Human\\nactivity recognition through the wall with commodity wi-ﬁ devices. I EEE\\nTransactions on Vehicular Technology 68(1), 306–319 (2018)\\n[17] Chowdhury, T.Z., Leung, C., Miao, C.Y.: Wihacs: Leveraging wiﬁ for hu man\\nactivity classiﬁcation using ofdm subcarriers’ correlation. In: 2017 IE EE Global\\nConference on Signal and Information Processing (GlobalSIP), pp. 338–342\\n(2017). IEEE\\n[18] Li, Z., Le Kernec, J., Abbasi, Q., Fioranelli, F., Yang, S., Romain, O. : Radar-\\nbased human activity recognition with adaptive thresholding towards r esource\\nconstrained platforms. Scientiﬁc Reports 13(1), 3473 (2023)\\n17', metadata={'source': 'sample.pdf', 'page': 17})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader(\"sample.pdf\")\n",
    "pdf_document=loader.load()\n",
    "pdf_document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### document transposition (dividing into chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='LLM Powered Autonomous Agents\\n    Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Fig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Fig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Fig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Chain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Fig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Fig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='In comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Fig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Maximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='LSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='HNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='FAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Fig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Fig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='They did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='ChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Fig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\", metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='API-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Fig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='This benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Case Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='One interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='inquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content=\"Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\", metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Fig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='GOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Commands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='You should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='},\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='},\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='}\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='FILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='You always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='pytest\\ndataclasses', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Conversatin samples:\\n[\\n  {\\n    \"role\": \"system\",', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='\"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='},\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='\"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Reliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Or\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "pdf_doc=text_splitter.split_documents(pdf_document)\n",
    "web_doc=text_splitter.split_documents(web_document)\n",
    "text_doc=text_splitter.split_documents(text_documents)\n",
    "pdf_doc\n",
    "web_doc\n",
    "text_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector embedding (converting text into vectors)\n",
    "### Vector Store (database for vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings  import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# db=Chroma.from_documents(pdf_doc[:15],OpenAIEmbeddings())\n",
    "db=Chroma.from_documents(pdf_doc[:15],OllamaEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wi-SensiNet: Through-Wall Human Activity\\nRecognition Based on WiFi Sensing\\nFu Xiaoyi\\xa0\\nSouthwest University\\nWang Chenlu\\xa0\\nSouthwest University\\nLi Shenglin\\xa0 \\n \\nSouthwest University\\nResearch Article\\nKeywords:  Human activity recognition, Deep learning, WiFi se nsing, Through wall,Channel state\\ninformation\\nPosted Date:  March 19th, 2024\\nDOI:  https://doi.org/10.21203/rs.3.rs-4106293/v1\\nLicense:  \\uf25e \\uf4e7 This work is licensed under a Creative Commons Att ribution 4.0 International License. \\xa0\\nRead Full License\\nAdditional Declarations:  No competing interests reported.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"what  are the name of authors?\"\n",
    "result=db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
